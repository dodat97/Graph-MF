---
title: "simulation-LDA-second-moment"
author: "Dat Do"
date: "2025-09-03"
output:
  workflowr::wflow_html:
    toc: true
    toc_float: true
---

The goal of this project is to visualize the relationship between factors in (Sparse) Matrix Factorization and topics in Latent Dirichlet Allocation (LDA). 


```{r setup, message=F, warning=F}
library(igraph)
library(MASS)
library(MCMCpack)
library(RColorBrewer)
set.seed(1)
```

Data generating distribution
```{r}


### Bifurcating tree L (each row is either 0 or exponentially distributed)
rMixPostiveManifold <- function(n, pis, rates){
  # Generate loading matrix from a bifurcating tree with 3 levels, 7 nodes, 4 path in total
  samples <- matrix(0, nrow=n, ncol=7)
  comp <- numeric(n)
  comp <- sample(1:4, size=n, replace = TRUE, prob = pis)
  path = matrix(c(1, 2, 3,
                  1, 2, 4,
                  1, 5, 6, 
                  1, 5, 7),
                nrow=4, ncol=3, byrow = TRUE)
  sorted_indx = c()
  for (i in 1:4){
    idx <- which(comp == i)
    sorted_indx <- c(sorted_indx, idx)
    if (length(idx) > 0){
      samples[idx, path[i, 1]] <- rexp(length(idx), rate=rates[1])
      samples[idx, path[i, 2]] <- rexp(length(idx), rate=rates[2])
      samples[idx, path[i, 3]] <- rexp(length(idx), rate=rates[3])
    }
  }
  L = samples[sorted_indx, ]
  return(L)
}

# L = rMixPostiveManifold(n=100, pis=rep(.25, 4), rates=rep(1, 3))
# View(L)



### admixture L (each row sum up to 1)
rMixSimplex <- function(n, pis, alphas){
  # Generate loading matrix from a bifurcating tree with 3 levels, 7 nodes, 4 path in total
  samples <- matrix(0, nrow=n, ncol=7)
  comp <- numeric(n)
  comp <- sample(1:4, size=n, replace = TRUE, prob = pis)
  path = matrix(c(1, 2, 3,
                  1, 2, 4,
                  1, 5, 6, 
                  1, 5, 7),
                nrow=4, ncol=3, byrow = TRUE)
  sorted_indx = c()
  for (i in 1:4){
    idx <- which(comp == i)
    sorted_indx <- c(sorted_indx, idx)
    if (length(idx) > 0){
      samples[idx, path[i, ]] <- rdirichlet(length(idx), alpha=alphas)
    }
  }
  L = samples[sorted_indx, ]
  return(L)
}


```

Admixture barplot of this loading matrix:
```{r}
## Generate true loading and plot
n = 700
L = rMixSimplex(n=n, pis=rep(.25, 4), alphas=c(.1, .1, .15))


colors7 <- brewer.pal(7, "Set1")   # qualitative, good for categories
colors7

colnames(L) <- paste0(1:7)

barplot(t(L[seq(1, n, by = 5), ]), beside = FALSE, col = colors7,
        border = NA, space = 0,
        xlab = "Individuals", ylab = "Proportion", las = 2)
legend("topright", legend = colnames(L), fill = colors7)
```

Here is the adjacency matrix formed by the second moments of $L$:

```{r}
## plot the graph
A = t(L) %*% L
diag(A) = 0
A = A / max(A) * 8
g <- graph_from_adjacency_matrix(A, mode = "undirected", weighted=TRUE)

# Plot it
plot(g,
     vertex.size = 30,
     vertex.label.cex = 1.2,
     vertex.color = "skyblue",
     edge.color = "gray40",
     edge.width = E(g)$weight,   # scale edge thickness by weight
     layout = layout_with_fr)

```


Now let us generate text counting data from this loading matrix (by first generate the word-topic matrix randomly then simulate via LDA model). Then we fit LDA model in to get the estimated $L$ matrix and plot its admixture barplot.

```{r}
## Generate document data from the true loading 
library(topicmodels)
library(tm)

V = 50
K = 7
beta <- matrix(0, nrow = K, ncol = V)
for (k in 1:K) {
  beta[k, ] <- rdirichlet(1, alpha = rep(0.1, V))  
}

doc_word_prob = L %*% beta
doc_mat = matrix(0, nrow = n, ncol = V)
for (d in 1:n){
  doc_mat[d, ] = rmultinom(1, size=100, prob=doc_word_prob[d, ])
}
### It is interesting that setting size small (=50) will give uniform posterior

print(doc_mat[3, ])

colnames(doc_mat) <- paste0("w", 1:V)
rownames(doc_mat) <- paste0("d", 1:n)
dtm <- as.DocumentTermMatrix(as.matrix(doc_mat), weighting = weightTf)

# Fit LDA with topicmodels (VEM)
lda_fit <- LDA(doc_mat, k = K, method = "VEM", control = list(seed = 123))

# Inspect results
L_hat <- posterior(lda_fit)$topics  # doc-topic distributions

barplot(t(L_hat[(1:n)%%5==0, ]), beside = FALSE, col = colors7,
        border = NA, space = 0,
        ylab = "Proportion", las = 2)

```

Plot the adajacency matrix

```{r}

A_hat = t(L_hat) %*% L_hat
diag(A_hat) = 0
A_adj = A_hat / max(A_hat) * 4
g <- graph_from_adjacency_matrix(A_adj, mode = "undirected", weighted=TRUE)

# Plot it
plot(g,
     vertex.size = 30,
     vertex.label.cex = 1.2,
     vertex.color = "skyblue",
     edge.color = "gray40",
     edge.width = E(g)$weight,   # scale edge thickness by weight
     layout = layout_with_fr)

```

The second moment of the estimated $L$ is not as sparse due to estimation error. However, there are many edges in this graph with weights almost zero. We can flatten the set of edges' weights to see

```{r}
hist(as.vector(A_hat))
```

We can use an Exponential Spike-and-Slab to separate noise from signal.

```{r}
exp_SnS <- function(x, pi_init=0.5, lambda1=50, lambda2_init=0.1,
                       tol=1e-8, max_iter=1000) {
  n <- length(x)
  
  # initialize
  pi <- pi_init
  lambda2 <- lambda2_init
  loglik_old <- -Inf
  
  for (iter in 1:max_iter) {
    # E-step: responsibilities
    dens1 <- lambda1 * exp(-lambda1 * x)
    dens2 <- lambda2 * exp(-lambda2 * x)
    w <- pi * dens2 / ((1 - pi) * dens1 + pi * dens2)
    
    # M-step
    pi <- mean(w)
    # lambda1 <- sum(1 - w) / sum((1 - w) * x)
    lambda2 <- sum(w) / sum(w * x)
    
    # log-likelihood
    ll <- sum(log((1 - pi) * dens1 + pi * dens2))
    if (abs(ll - loglik_old) < tol) break
    loglik_old <- ll
  }
  
  list(pi=pi, lambda1=lambda1, lambda2=lambda2, loglik=loglik_old, iter=iter)
}

V = L_hat

lambda1 = 200  ## for spike component
for (k in 1:7){
  x = L_hat[ ,k]
  ret = exp_SnS(x, lambda1 = lambda1, lambda2_init = 0.1)
  pi = ret$pi
  lambda2 = ret$lambda2
  x_range <- seq(0, max(x), length.out=200)
  y1 <- (1-pi) * dexp(x_range, rate=lambda1)
  y2 <- pi * dexp(x_range, rate=lambda2)
  # histogram with density scale
  hist(x, breaks=50, freq=FALSE, col="lightgray",
       border="white", main=paste0("Factor ", k),
       xlab="x", ylab="Density")
  # add pdf curves
  lines(x_range, y1, col="blue", lwd=2)
  lines(x_range, y2, col="red", lwd=2, lty=2)
  
  thresh = x_range[sum(y1 > y2)]
  V[, k] = (L_hat[, k] > thresh) * L_hat[, k]
  # legend
  legend("topright", legend=c(paste0("Exp(", lambda1, ")"),
                              paste0("Exp(", lambda2, ")")),
         col=c("blue", "red"), lwd=2, lty=c(1,2))
}


```

```{r}
A_hat = t(V) %*% V
diag(A_hat) = 0
A_adj = A_hat / max(A_hat) * 4
g <- graph_from_adjacency_matrix(A_adj, mode = "undirected", weighted=TRUE)

# Plot it
plot(g,
     vertex.size = 30,
     vertex.label.cex = 1.2,
     vertex.color = "skyblue",
     edge.color = "gray40",
     edge.width = E(g)$weight,   # scale edge thickness by weight
     layout = layout_with_fr)
```
We almost recover the true graph.

Now let's challenge it by over-fitting the topic model.

```{r}
lda_fit <- LDA(doc_mat, k = 8, method = "VEM", control = list(seed = 123))
L_hat <- posterior(lda_fit)$topics  # doc-topic distributions
colors8 = brewer.pal(8, "Set1") 
barplot(t(L_hat[(1:n)%%5==0, ]), beside = FALSE, col = colors7,
        border = NA, space = 0,
        ylab = "Proportion", las = 2)

A_hat = t(L_hat) %*% L_hat
diag(A_hat) = 0
A_adj = A_hat / max(A_hat) * 8
g <- graph_from_adjacency_matrix(A_adj, mode = "undirected", weighted=TRUE)

# Plot it
plot(g,
     vertex.size = 30,
     vertex.label.cex = 1.2,
     vertex.color = "skyblue",
     edge.color = "gray40",
     edge.width = E(g)$weight,   # scale edge thickness by weight
     layout = layout_with_fr)

```


Let's truncate the result and plot again
```{r}
lambda1 = 200  ## for spike component
V = L_hat
for (k in 1:8){
  x = L_hat[ ,k]
  ret = exp_SnS(x, lambda1 = lambda1, lambda2_init = 0.1)
  pi = ret$pi
  lambda2 = ret$lambda2
  x_range <- seq(0, max(x), length.out=200)
  y1 <- (1-pi) * dexp(x_range, rate=lambda1)
  y2 <- pi * dexp(x_range, rate=lambda2)
  # histogram with density scale
  hist(x, breaks=50, freq=FALSE, col="lightgray",
       border="white", main=paste0("Factor ", k),
       xlab="x", ylab="Density")
  # add pdf curves
  lines(x_range, y1, col="blue", lwd=2)
  lines(x_range, y2, col="red", lwd=2, lty=2)
  
  thresh = x_range[sum(y1 > y2)]
  V[, k] = (L_hat[, k] > thresh) * L_hat[, k]
  # legend
  legend("topright", legend=c(paste0("Exp(", lambda1, ")"),
                              paste0("Exp(", lambda2, ")")),
         col=c("blue", "red"), lwd=2, lty=c(1,2))
}

A_hat = t(V) %*% V
diag(A_hat) = 0
A_adj = A_hat / max(A_hat) * 4
g <- graph_from_adjacency_matrix(A_adj, mode = "undirected", weighted=TRUE)

# Plot it
plot(g,
     vertex.size = 30,
     vertex.label.cex = 1.2,
     vertex.color = "skyblue",
     edge.color = "gray40",
     edge.width = E(g)$weight,   # scale edge thickness by weight
     layout = layout_with_fr)
```

We can see that one over-fitted topic have very similar behavior with a topic. How to merge them?
